# this is setting file for the scraler

# which fetcher you want to use 
fetcher=org.easyGoingCrawler.fetcher.HttpFetcher

# which DocWriter you want to use 
docwriter=org.easyGoingCrawler.docWriter.MirrorWriter

# which extractor you want to use 
extractor=org.easyGoingCrawler.extractor.HTMLExtractor

urlscheduler=org.easyGoingCrawler.urlscheduler.RotateHostURLScheduler1

interval = 2000

# seeds file
seeds =conf/seeds1.txt

baseDirectory = C:/crawledHTML/

AUDIT_LOG_FILE_PATH = log/log.log
MAX_BACKUP_INDEX=2
MAX_FILE_SIZE=100000

# crawler pool size
POOLSIZE = 20